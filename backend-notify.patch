diff --git a/app/routes/analyze.py b/app/routes/analyze.py
index 0000000..1111111 100644
--- a/app/routes/analyze.py
+++ b/app/routes/analyze.py
@@ -6,11 +6,13 @@ from enum import Enum
 
 from app.utils.logger import logger
 from app.services.pdf_extractor import extract_pdf_text, extract_pdf_pages
 from app.services.text_cleaner import clean_text
 from app.services.chunker import chunk_text
 from app.services.classifier import classify_document
 from app.services.structure_extractor import extract_structure
 from app.config import UPLOAD_DIR
+from app.services.language import detect_language
+from app.services.notifier import notify_admin
 
 router = APIRouter()
@@ -72,10 +74,18 @@ async def analyze_document(file_id: str):
     try:
         set_status(file_id, TaskStatus.EXTRACTING)
         pages = await extract_pdf_pages(file_path)
         logger.info(f"[ANALYZE] extract_pdf_pages OK: {len(pages)} pages")
     except Exception as e:
         logger.exception(f"[ANALYZE] extract_pdf_pages failed: {e}")
         pages = []
+        # уведомим админа, что упал разбор страниц
+        try:
+            await notify_admin(
+                f"❌ ANALYZE ERROR (extract_pdf_pages)\n"
+                f"file_id={file_id}\n"
+                f"{e}"
+            )
+        except Exception as ne:
+            logger.error(f"[NOTIFY] Failed to notify admin (pages): {ne}")
@@ -86,10 +96,20 @@ async def analyze_document(file_id: str):
     try:
         raw_text = await extract_pdf_text(file_path)
         logger.info(f"[ANALYZE] extract_pdf_text OK ({len(raw_text)} chars)")
     except Exception as e:
         logger.error(f"[ANALYZE] extract_pdf_text crashed: {e}")
         raw_text = ""
+        # уведомим админа, что упала основная текстовая экстракция
+        try:
+            await notify_admin(
+                f"❌ ANALYZE ERROR (extract_pdf_text)\n"
+                f"file_id={file_id}\n"
+                f"{e}"
+            )
+        except Exception as ne:
+            logger.error(f"[NOTIFY] Failed to notify admin (text): {ne}")
@@ -99,10 +119,20 @@ async def analyze_document(file_id: str):
         logger.warning("[ANALYZE] Text empty → fallback to pages[]")
         if pages:
             raw_text = "\n\n".join([p.get("text", "") for p in pages])
 
     if not raw_text.strip():
         set_status(file_id, TaskStatus.ERROR)
+        # вообще не удалось получить текст
+        try:
+            await notify_admin(
+                f"❌ ANALYZE ERROR (no text)\n"
+                f"file_id={file_id}\n"
+                f"Both extract_pdf_text and pages[] fallback are empty."
+            )
+        except Exception as ne:
+            logger.error(f"[NOTIFY] Failed to notify admin (no text): {ne}")
         raise HTTPException(status_code=500, detail="Failed to extract text from document")
@@ -113,10 +143,20 @@ async def analyze_document(file_id: str):
     # 4) Clean text
     # -----------------------------------------------------------
     cleaned = clean_text(raw_text)
     if not cleaned.strip():
         set_status(file_id, TaskStatus.ERROR)
+        try:
+            await notify_admin(
+                f"❌ ANALYZE ERROR (clean_text)\n"
+                f"file_id={file_id}\n"
+                f"Cleaner returned empty string."
+            )
+        except Exception as ne:
+            logger.error(f"[NOTIFY] Failed to notify admin (clean_text): {ne}")
         raise HTTPException(status_code=500, detail="Text cleaning failed")
@@ -124,13 +164,13 @@ async def analyze_document(file_id: str):
     # -----------------------------------------------------------
     # 4.1 NEW — Language detection
     # -----------------------------------------------------------
     try:
         language = detect_language(cleaned)
         logger.info(f"[ANALYZE] Language detected → {language}")
     except Exception as e:
         logger.error(f"[ANALYZE] Language detection failed: {e}")
         language = "en"
@@ -141,10 +181,20 @@ async def analyze_document(file_id: str):
     set_status(file_id, TaskStatus.CHUNKING)
 
     chunks = chunk_text(cleaned, max_chars=2000, overlap=200)
     if not chunks:
         set_status(file_id, TaskStatus.ERROR)
+        try:
+            await notify_admin(
+                f"❌ ANALYZE ERROR (chunk_text)\n"
+                f"file_id={file_id}\n"
+                f"No chunks produced."
+            )
+        except Exception as ne:
+            logger.error(f"[NOTIFY] Failed to notify admin (chunk_text): {ne}")
         raise HTTPException(status_code=500, detail="Chunking failed")
@@ -156,10 +206,18 @@ async def analyze_document(file_id: str):
 
     try:
         analysis = classify_document(chunks[0])
     except Exception as e:
         logger.error(f"[ANALYZE] classify_document failed: {e}")
         set_status(file_id, TaskStatus.ERROR)
+        try:
+            await notify_admin(
+                f"❌ ANALYZE ERROR (classify_document)\n"
+                f"file_id={file_id}\n"
+                f"{e}"
+            )
+        except Exception as ne:
+            logger.error(f"[NOTIFY] Failed to notify admin (classify): {ne}")
         raise HTTPException(status_code=500, detail="Document classification failed")
@@ -172,10 +230,18 @@ async def analyze_document(file_id: str):
     try:
         structure = extract_structure(file_path) or []
         logger.info(f"[ANALYZE] Structure extracted: {len(structure)} units")
     except Exception as e:
         logger.error(f"[ANALYZE] Structure extractor failed: {e}")
         structure = []
+        try:
+            await notify_admin(
+                f"⚠️ ANALYZE WARNING (structure)\n"
+                f"file_id={file_id}\n"
+                f"{e}"
+            )
+        except Exception as ne:
+            logger.error(f"[NOTIFY] Failed to notify admin (structure): {ne}")
